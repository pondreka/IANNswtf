# -*- coding: utf-8 -*-
"""Autoencoders.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yBqtm8ys1LSU6GKn8yVEN-EMpcu2chAV
"""

import tensorflow_datasets as tfds
import tensorflow as tf
import matplotlib.pyplot as plt

# ----------- task 1 "Data set" ------------------
train_ds, test_ds = tfds.load(
    "mnist", split=["train", "test"], as_supervised=True
)
test_imgs = test_ds.take(10)


def prepare_data(data, noise: float = 0.3):
    # convert data from uint8 to float32
    data = data.map(lambda img, target: tf.cast(img, tf.float32))
    # sloppy input normalization, just bringing image values from range [0, 255] to [-1, 1]
    data = data.map(lambda img: img / 255)
    # Add noise to image and create target vector
    data = data.map(
        lambda img: (
            img
            + tf.random.normal(shape=img.shape, mean=0.0, stddev=1.0 * noise),
            img,
        )
    )
    # Scaling of the noisy image
    data = data.map(
        lambda img, target: (
            tf.clip_by_value(img, clip_value_min=0, clip_value_max=1),
            target,
        )
    )
    return data


def prepare_mnist_data(mnist):
    mnist = mnist.apply(prepare_data)
    # cache this progress in memory, as there is no need to redo it; it is deterministic after all
    mnist = mnist.cache()
    # shuffle, batch, prefetch
    mnist = mnist.shuffle(1000)
    mnist = mnist.batch(64)
    mnist = mnist.prefetch(10)
    # return preprocessed dataset
    return mnist


train_ds = train_ds.apply(prepare_mnist_data)
test_ds = test_ds.apply(prepare_mnist_data)
test_imgs = (
    test_imgs.apply(prepare_data)
    .map(lambda img, target: img)
    .map(lambda img: tf.expand_dims(img, 0))
)

for x, t in train_ds:
    n = 10  # How many digits we will display
    plt.figure(figsize=(20, 4))
    for i in range(n):
        # Display noisy image
        ax = plt.subplot(2, n, i + 1)
        plt.imshow(tf.squeeze(x[i]))
        plt.gray()
        ax.get_xaxis().set_visible(False)
        ax.get_yaxis().set_visible(False)

        # Display reconstruction
        ax = plt.subplot(2, n, i + 1 + n)
        plt.imshow(tf.squeeze(t[i]))
        plt.gray()
        ax.get_xaxis().set_visible(False)
        ax.get_yaxis().set_visible(False)
    plt.show()
    break


# ------------ task 2 "Model" --------------
# ------------ task 2.1 "Convolutional Autoencoder" ----------------


class Encoder(tf.keras.Model):
    def __init__(self, latent_dim: int = 10):
        super(Encoder, self).__init__()

        self.all_layers = [
            tf.keras.layers.Conv2D(
                filters=32, kernel_size=(3, 3), strides=(2, 2), padding="same",
            ),
            tf.keras.layers.BatchNormalization(),
            tf.keras.layers.Activation(tf.nn.relu),
            tf.keras.layers.Conv2D(
                filters=64, kernel_size=(3, 3), strides=(2, 2), padding="same",
            ),
            tf.keras.layers.BatchNormalization(),
            tf.keras.layers.Activation(tf.nn.relu),
            tf.keras.layers.Flatten(),
            tf.keras.layers.Dense(7 * 7 * 64),
            tf.keras.layers.BatchNormalization(),
            tf.keras.layers.Activation(tf.nn.relu),
            tf.keras.layers.Dense(latent_dim, activation=tf.nn.relu),
        ]

    @tf.function
    def call(self, x: tf.Tensor, training: bool = False) -> tf.Tensor:

        for layer in self.all_layers:
            try:
                x = layer(x, training=training)
            except:
                x = layer(x)

        return x


from tensorflow.python.ops.gen_math_ops import sigmoid


class Decoder(tf.keras.Model):
    def __init__(self):
        super(Decoder, self).__init__()

        self.all_layers = [
            tf.keras.layers.Dense(7 * 7 * 64),
            tf.keras.layers.BatchNormalization(),
            tf.keras.layers.Activation(tf.nn.relu),
            tf.keras.layers.Reshape((7, 7, 64)),
            tf.keras.layers.Conv2DTranspose(
                filters=64, kernel_size=(3, 3), strides=(2, 2), padding="same"
            ),
            tf.keras.layers.BatchNormalization(),
            tf.keras.layers.Activation(tf.nn.relu),
            tf.keras.layers.Conv2DTranspose(
                filters=32, kernel_size=(3, 3), strides=(2, 2), padding="same"
            ),
            tf.keras.layers.BatchNormalization(),
            tf.keras.layers.Activation(tf.nn.relu),
            tf.keras.layers.Conv2D(
                filters=1,
                kernel_size=(3, 3),
                strides=(1, 1),
                padding="same",
                activation=tf.nn.sigmoid,
            ),
        ]

    @tf.function
    def call(self, x: tf.Tensor, training: bool = False) -> tf.Tensor:

        for layer in self.all_layers:
            try:
                x = layer(x, training=training)
            except:
                x = layer(x)

        return x


class Autoencoder(tf.keras.Model):
    def __init__(self, latent_dim: int = 10):
        super(Autoencoder, self).__init__()

        self.encoder = Encoder(latent_dim)
        self.decoder = Decoder()

    @tf.function
    def call(self, inputs: tf.Tensor, training: bool = False) -> tf.Tensor:
        out_encoder = self.encoder(inputs, training=training)
        out_decoder = self.decoder(out_encoder, training=training)

        return out_decoder


autoencoder = Autoencoder()

# --------------- task 3 " Training" ---------------
import numpy as np


def train_step(
    model: tf.keras.Model,
    data: tf.Tensor,
    target: tf.Tensor,
    loss_function: tf.keras.losses,
    optimizer: tf.keras.optimizers,
    training: bool = True,
):
    """Training iteration over one input data.

    Args:
        model: Model to train.
        data: Data used to calculate the predictions.
        target: Targets for the loss and accuracy calculation.
        loss_function: Function used to calculate the loss.
        optimizer: an optimizer to apply with the gradient

    Returns:
        (tf.Tensor, tf.Tensor): A tuple with the calculated loss and
            accuracy.
    """

    with tf.GradientTape() as tape:
        prediction = model(data, training)
        loss = loss_function(target, prediction)
    gradients = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))

    return loss


def test(
    model: tf.keras.Model,
    test_data: tf.Tensor,
    loss_function: tf.keras.losses,
    training: bool = False,
):
    """Test iteration over all test data.

    Args:
        model: Model to train.
        test_data: Dataset to test with the model.
        loss_function: Function used to calculate the loss.

    Returns:
        (float, float): A tuple with the calculated loss and accuracy
    """
    test_loss_aggregator = []

    for data, target in test_data:
        prediction = model(data)

        test_loss = loss_function(target, prediction)
        test_loss_aggregator.append(test_loss.numpy())

    test_loss = tf.reduce_mean(test_loss_aggregator)

    return test_loss


def visualization(train_losses, test_losses):
    plt.figure()
    (line1,) = plt.plot(train_losses)
    (line2,) = plt.plot(test_losses)
    plt.xlabel("Training steps")
    plt.ylabel("Loss")
    plt.legend((line1, line2), ("training", "test"))
    plt.show()


def training(
    model: tf.keras.Model,
    train_ds: tf.Tensor,
    test_ds: tf.Tensor,
    num_epochs: int = 5,
    learning_rate: int = 0.001,
):
    loss = tf.keras.losses.MeanSquaredError()
    optimizer = tf.keras.optimizers.Adam(learning_rate)

    # lists of tensors
    train_losses: list = []
    test_losses: list = []

    # testing train dataset once before we starting the training
    train_loss = test(model, train_ds, loss)
    train_losses.append(train_loss)

    # testing test dataset once before we starting the training
    test_loss = test(model, test_ds, loss)
    test_losses.append(test_loss)

    # We train for num_epochs epochs.
    for epoch in range(num_epochs):
        print("Epoch: " + str(epoch))

        # training (and checking in with training)
        epoch_loss_agg = []
        for inp, target in train_ds:
            train_loss = train_step(
                model, inp, target, loss, optimizer, training=True
            )
            epoch_loss_agg.append(train_loss)

        # track training loss
        train_losses.append(tf.reduce_mean(epoch_loss_agg))

        # testing, so we can track valid accuracy and valid loss
        test_loss = test(model, test_ds, loss)
        test_losses.append(test_loss)

        visualization(train_losses, test_losses)

        plt.figure(figsize=(20, 4))
        for i, inp in enumerate(test_imgs):
            n = 10
            decoded_img = model(inp)

            # Display noisy image
            ax = plt.subplot(2, n, i + 1)
            plt.imshow(tf.squeeze(inp))
            plt.gray()
            ax.get_xaxis().set_visible(False)
            ax.get_yaxis().set_visible(False)

            # Display reconstruction
            ax = plt.subplot(2, n, i + 1 + n)
            plt.imshow(tf.squeeze(decoded_img))
            plt.gray()
            ax.get_xaxis().set_visible(False)
            ax.get_yaxis().set_visible(False)

        plt.show()


training(model=autoencoder, train_ds=train_ds, test_ds=test_ds)

autoencoder.summary()

# --------------- task 4 " Latent Space Analysis" ---------------
from sklearn.manifold import TSNE

label_names = [str(l) for l in np.arange(10)]

(
    (train_x, train_labels),
    (test_x, test_labels),
) = tf.keras.datasets.mnist.load_data()

img = test_x[:1000]
labels = test_labels[:1000]
img = tf.expand_dims(tf.cast(img, tf.float32) / 255.0, -1)

encoded = autoencoder.encoder.predict(img)
encoded = np.reshape(encoded, (encoded.shape[0], -1))

embedded = TSNE(n_components=2, perplexity=20).fit_transform(encoded)

fig = plt.figure(figsize=(10, 10))
scatter = plt.scatter(embedded[:, 0], embedded[:, 1], c=labels, cmap="tab10")
plt.legend(handles=scatter.legend_elements()[0], labels=label_names)

model_2dim = Autoencoder(2)
training(model_2dim, train_ds, test_ds)

encoded = model_2dim.encoder.predict(img)

fig = plt.figure(figsize=(10, 10))
scatter = plt.scatter(encoded[:, 0], encoded[:, 1], c=labels, cmap="tab10")
plt.legend(handles=scatter.legend_elements()[0], labels=label_names)

imgs = tf.convert_to_tensor([img[2], img[10]])
encoded = autoencoder.encoder(imgs)
encoded_1, encoded_2 = encoded[0], encoded[1]

num_ints = 20
int_factors = np.linspace(0, 1, num_ints)
ints = []
for i in int_factors:
    ints.append(encoded_1 * i + encoded_2 * (1 - i))

decoded_imgs = autoencoder.decoder(tf.convert_to_tensor(ints))

fig = plt.figure(figsize=(2 * num_ints, 4))
for i in range(num_ints):
    plt.subplot(1, num_ints, i + 1)
    plt.imshow(tf.squeeze(decoded_imgs[i]))
    plt.axis("off")
plt.show()
